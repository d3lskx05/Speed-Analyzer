# app.py
import streamlit as st
import time
import os
import psutil
import requests
import functools
import math
import gdown
import zipfile
import shutil
import pandas as pd
import plotly.express as px
from sentence_transformers import SentenceTransformer
import torch

# ---------- Helper utilities ----------

def sizeof_fmt(num, suffix="B"):
    for unit in ["", "K", "M", "G", "T", "P"]:
        if abs(num) < 1024.0:
            return f"{num:3.1f}{unit}{suffix}"
        num /= 1024.0
    return f"{num:.1f}P{suffix}"

def get_ram_usage_mb():
    proc = psutil.Process(os.getpid())
    return proc.memory_info().rss / (1024 ** 2)

def path_size_bytes(path):
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for f in filenames:
            try:
                fp = os.path.join(dirpath, f)
                total += os.path.getsize(fp)
            except Exception:
                pass
    return total

def try_import(name):
    try:
        return __import__(name)
    except Exception:
        return None

# ---------- HF API helpers ----------

def hf_model_info(model_name, token=None):
    try:
        headers = {}
        if token:
            headers["Authorization"] = f"Bearer {token}"
        url = f"https://huggingface.co/api/models/{model_name}"
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return None

def hf_model_files(model_name, token=None):
    try:
        headers = {}
        if token:
            headers["Authorization"] = f"Bearer {token}"
        url = f"https://huggingface.co/api/models/{model_name}/revision/main/files"
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return None

# ---------- Model loading (cached) ----------

@functools.lru_cache(maxsize=8)
def load_model_cached(path_or_name: str, from_hf: bool = True, hf_token: str = None):
    if from_hf and hf_token:
        model = SentenceTransformer(path_or_name, use_auth_token=hf_token)
    else:
        model = SentenceTransformer(path_or_name)
    return model

# ---------- Introspection helpers ----------

def inspect_model_info(model):
    info = {}
    try:
        emb = model.encode("—Ç–µ—Å—Ç", convert_to_tensor=True)
        info["dim"] = int(emb.shape[-1])
    except Exception:
        info["dim"] = None
    try:
        total = 0
        for p in model.parameters():
            total += p.numel()
        info["params"] = int(total)
    except Exception:
        info["params"] = None

    num_layers = None
    try:
        first = None
        try:
            first = model._first_module()
        except Exception:
            mods = list(model._modules.values())
            if mods:
                first = mods[0]
        if first is not None:
            auto_model = getattr(first, "auto_model", None) or getattr(first, "model", None) or getattr(first, "hf_model", None)
            if auto_model is not None:
                cfg = getattr(auto_model, "config", None)
                if cfg is not None:
                    num_layers = getattr(cfg, "num_hidden_layers", None) or getattr(cfg, "num_layers", None)
    except Exception:
        num_layers = None
    info["num_layers"] = int(num_layers) if num_layers is not None else None

    info["batch_optimized"] = True
    bnb = try_import("bitsandbytes")
    info["quantization_bitsandbytes_available"] = bool(bnb)
    info["fp16_cuda_available"] = torch.cuda.is_available()
    try:
        info["model_cache_folder"] = getattr(model, "cache_folder", None)
    except Exception:
        info["model_cache_folder"] = None
    return info

# ---------- Benchmarking ----------

def benchmark_model(model_name_or_path, source="HF", n_queries=10, text_length="short", hf_token=None):
    m = {}
    m["model_id_or_path"] = model_name_or_path
    m["source"] = source
    hf_meta = None
    if source == "HF":
        hf_meta = hf_model_info(model_name_or_path, token=hf_token)
    t0 = time.time()
    try:
        model = load_model_cached(model_name_or_path, from_hf=(source=="HF"), hf_token=hf_token)
        load_time = time.time() - t0
        m["load_time_sec"] = round(load_time, 3)
    except Exception as e:
        m["error"] = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}"
        return m
    try:
        m["ram_after_load_mb"] = round(get_ram_usage_mb(), 2)
    except Exception:
        m["ram_after_load_mb"] = None

    try:
        size_mb = None
        if source != "HF" and os.path.isdir(model_name_or_path):
            size_mb = path_size_bytes(model_name_or_path) / (1024*1024)
        else:
            cache_folder = getattr(model, "cache_folder", None)
            if cache_folder:
                sub = model_name_or_path.replace("/", "_")
                candidate = os.path.join(cache_folder, sub)
                if os.path.isdir(candidate):
                    size_mb = path_size_bytes(candidate) / (1024*1024)
                else:
                    size_mb = path_size_bytes(cache_folder) / (1024*1024)
        if size_mb is None and source == "HF":
            files = hf_model_files(model_name_or_path, token=hf_token)
            if isinstance(files, list):
                total = sum([f.get("size",0) if isinstance(f, dict) else 0 for f in files])
                size_mb = total / (1024*1024) if total>0 else None
        m["model_size_mb"] = round(size_mb,2) if size_mb else None
    except Exception:
        m["model_size_mb"] = None

    try:
        info = inspect_model_info(model)
        m.update({
            "embedding_dim": info.get("dim"),
            "num_parameters": int(info.get("params")) if info.get("params") else None,
            "num_layers": info.get("num_layers"),
            "batch_optimized": info.get("batch_optimized"),
            "quantization_bitsandbytes_available": info.get("quantization_bitsandbytes_available"),
            "fp16_cuda_available": info.get("fp16_cuda_available")
        })
    except Exception:
        pass

    if hf_meta:
        try:
            m["hf_author"] = hf_meta.get("author")
            m["hf_lastModified"] = hf_meta.get("lastModified")
            tags = hf_meta.get("tags") or []
            langs = hf_meta.get("languages") or []
            m["hf_tags"] = ", ".join(tags) if tags else None
            m["hf_languages"] = ", ".join(langs) if langs else None
        except Exception:
            pass

    if text_length=="short":
        sample_text="–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
    elif text_length=="medium":
        sample_text="–≠—Ç–æ —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–æ–¥–µ–ª—å—é."
    else:
        sample_text=" ".join(["–î–ª–∏–Ω–Ω—ã–π"]*200)

    try:
        _ = model.encode("—Ç—ë—Å—Ç", convert_to_tensor=True)
    except Exception:
        pass
    try:
        t1 = time.time()
        _ = model.encode(sample_text, convert_to_tensor=True)
        t_single = time.time() - t1
        m["time_single_ms"] = round(t_single*1000,3)
    except Exception:
        m["time_single_ms"] = None

    try:
        texts=[sample_text]*max(1,int(n_queries))
        t2=time.time()
        _=model.encode(texts, convert_to_tensor=True)
        t_batch=time.time()-t2
        m["time_batch_sec"]=round(t_batch,3)
        m["avg_per_query_ms"]=round((t_batch/len(texts))*1000,3)
    except Exception:
        m["time_batch_sec"]=None
        m["avg_per_query_ms"]=None

    try:
        cpu_before=psutil.cpu_percent(interval=None)
        _ = model.encode([sample_text]*5, convert_to_tensor=True)
        cpu_after=psutil.cpu_percent(interval=None)
        m["cpu_percent_sample"]=round((cpu_before+cpu_after)/2,2)
    except Exception:
        m["cpu_percent_sample"]=None

    return m

# ---------- Optimization tips ----------

def optimization_tips(result):
    tips=[]
    size=result.get("model_size_mb")
    params=result.get("num_parameters")
    ram=result.get("ram_after_load_mb")
    dim=result.get("embedding_dim")
    quant=result.get("quantization_bitsandbytes_available")
    fp16=result.get("fp16_cuda_available")

    if size and size>700:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å –±–æ–ª—å—à–∞—è (>700MB). –ù–∞ Streamlit Free –≤–æ–∑–º–æ–∂–Ω—ã OOM/–ø–∞–¥–µ–Ω–∏—è.")
    elif size and size>300:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (300‚Äì700MB). –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥–∏ –∑–∞ RAM.")
    elif size:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è ‚Äî –¥–æ–ª–∂–Ω–∞ –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –∏ —Ä–∞–±–æ—Ç–∞—Ç—å –ª—É—á—à–µ –Ω–∞ Free tier.")

    if params and params>200_000_000:
        tips.append("‚Ä¢ –ú–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (>200M) ‚Äî –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ CPU.")
    elif params and params>80_000_000:
        tips.append("‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —É–º–µ—Ä–µ–Ω–Ω–æ–µ (80‚Äì200M). –¢–µ—Å—Ç–∏—Ä—É–π batch_size.")

    if ram and ram>900:
        tips.append("‚Ä¢ –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª—å –∑–∞–Ω–∏–º–∞–µ—Ç –º–Ω–æ–≥–æ RAM (>900MB).")
    if quant:
        tips.append("‚Ä¢ bitsandbytes –¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å INT8/4bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ.")
    else:
        tips.append("‚Ä¢ bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.")
    if fp16:
        tips.append("‚Ä¢ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed-precision (fp16).")
    else:
        tips.append("‚Ä¢ CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî fp16 –Ω–µ –ø–æ–º–æ–∂–µ—Ç –Ω–∞ —ç—Ç–æ–π –º–∞—à–∏–Ω–µ.")
    if dim and dim>=1024:
        tips.append("‚Ä¢ –ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (>=1024) ‚Äî –ø–æ–≤—ã—à–µ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏.")
    elif dim and dim<=384:
        tips.append("‚Ä¢ –ú–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (<=384) ‚Äî —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å, —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ.")

    if size and size>700 or (ram and ram>900) or (params and params>200_000_000):
        tips.append("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ <500MB –∏–ª–∏ quantized/distilled –≤–µ—Ä—Å–∏–∏.")
    else:
        tips.append("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –º–æ–¥–µ–ª—å –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è Streamlit Free —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é.")

    return "\n".join(tips)

# ---------- Download GDrive ----------

def download_gdrive_model(file_id, dest_folder):
    os.makedirs(dest_folder, exist_ok=True)
    zip_path=os.path.join(dest_folder,"model.zip")
    url=f"https://drive.google.com/uc?id={file_id}"
    gdown.download(url, output=zip_path, quiet=False)
    with zipfile.ZipFile(zip_path,'r') as zip_ref:
        zip_ref.extractall(dest_folder)
    return dest_folder

# ---------- Cleanup tmp with logging ----------

def cleanup_tmp(folder_list):
    for folder in folder_list:
        if os.path.exists(folder):
            start=time.time()
            size_before=path_size_bytes(folder)
            try:
                shutil.rmtree(folder)
                elapsed=time.time()-start
                st.info(f"–û—á–∏—Å—Ç–∫–∞ {folder}: {sizeof_fmt(size_before)}, –≤—Ä–µ–º—è {elapsed:.2f}s")
            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {folder}: {e}")

# ---------- Normalize results ----------

def normalize_result(res):
    keys=["load_time_sec","ram_after_load_mb","time_single_ms","time_batch_sec",
          "avg_per_query_ms","model_size_mb","embedding_dim","num_parameters"]
    if not res or "error" in res:
        return {k: None for k in keys}
    return {k: res.get(k,None) for k in keys}

# ---------- Streamlit UI ----------

st.set_page_config(layout="wide", page_title="Model Benchmark & Optimizer")
st.title("üîé Model Benchmark & Optimizer (HF + GDrive)")

mode=st.radio("–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:", ["Single","A/B —Ç–µ—Å—Ç"])

if mode=="Single":
    col1,col2=st.columns([1,2])
    with col1:
        source=st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏:", ["HuggingFace","GDrive"])
        if source=="HuggingFace":
            model_input=st.text_input("HF model id:", value="deepvk/USER-bge-m3")
            use_token=st.checkbox("–ü—Ä–∏–≤–∞—Ç–Ω—ã–π HF (use token from st.secrets['HUGGINGFACE_TOKEN'])", value=False)
            hf_token=st.secrets.get("HUGGINGFACE_TOKEN") if use_token else None
        else:
            model_input=st.text_input("GDrive File ID –º–æ–¥–µ–ª–∏:", value="1bZoykt0Sj2GRPvLRC_3Z7Wt8g4AGT34R")
            hf_token=None
        n_queries=st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ (batch) –¥–ª—è —Ç–µ—Å—Ç–∞:", min_value=1,max_value=1000,value=10,step=1)
        text_len=st.selectbox("–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞:", ["short","medium","long"])
        run_btn=st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç")

    with col2:
        st.markdown("**–°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**")
        if "bench_results" not in st.session_state:
            st.session_state.bench_results=[]

    if run_btn:
        with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±–µ–Ω—á–º–∞—Ä–∫..."):
            try:
                if source=="GDrive":
                    model_path=download_gdrive_model(model_input,"/tmp/model_single")
                    res=benchmark_model(model_path, source="Local", n_queries=n_queries, text_length=text_len)
                else:
                    res=benchmark_model(model_input, source="HF", n_queries=n_queries, text_length=text_len, hf_token=hf_token)
                res["timestamp"]=time.strftime("%Y-%m-%d %H:%M:%S")
                st.session_state.bench_results.append(res)
                st.success("–¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω")
                cleanup_tmp(["/tmp/model_single"])
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")

if mode=="A/B —Ç–µ—Å—Ç":
    # --- A/B UI ---
    colA,colB=st.columns(2)
    with colA:
        source_a=st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏ A:", ["HF","GDrive"], key="sourceA")
        if source_a=="HF":
            modelA=st.text_input("–ú–æ–¥–µ–ª—å A (HF ID):","deepvk/USER-bge-m3", key="modelA")
            hf_tokenA=st.text_input("HF Token –º–æ–¥–µ–ª–∏ A (–µ—Å–ª–∏ –ø—Ä–∏–≤–∞—Ç–Ω–∞—è):", type="password", key="hfA")
        else:
            modelA_file_id=st.text_input("GDrive File ID –º–æ–¥–µ–ª–∏ A:","1bZoykt0Sj2GRPvLRC_3Z7Wt8g4AGT34R", key="gdriveA")
    with colB:
        source_b=st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏ B:", ["HF","GDrive"], key="sourceB")
        if source_b=="HF":
            modelB=st.text_input("–ú–æ–¥–µ–ª—å B (HF ID):","sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", key="modelB")
            hf_tokenB=st.text_input("HF Token –º–æ–¥–µ–ª–∏ B (–µ—Å–ª–∏ –ø—Ä–∏–≤–∞—Ç–Ω–∞—è):", type="password", key="hfB")
        else:
            modelB_file_id=st.text_input("GDrive File ID –º–æ–¥–µ–ª–∏ B:","1bZoykt0Sj2GRPvLRC_3Z7Wt8g4AGT34R", key="gdriveB")
    n_queries_ab=st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:", min_value=1,max_value=500,value=10,key="n_queries_ab")
    text_len_ab=st.selectbox("–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞:", ["short","medium","long"], key="text_len_ab")

    if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å A/B —Ç–µ—Å—Ç"):
        with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ–º A/B —Ç–µ—Å—Ç..."):
            try:
                # –º–æ–¥–µ–ª—å A
                if source_a=="GDrive":
                    modelA_path=download_gdrive_model(modelA_file_id,"/tmp/modelA")
                    resA=benchmark_model(modelA_path, source="Local", n_queries=n_queries_ab, text_length=text_len_ab)
                else:
                    resA=benchmark_model(modelA, source="HF", n_queries=n_queries_ab, text_length=text_len_ab, hf_token=hf_tokenA)
                # –º–æ–¥–µ–ª—å B
                if source_b=="GDrive":
                    modelB_path=download_gdrive_model(modelB_file_id,"/tmp/modelB")
                    resB=benchmark_model(modelB_path, source="Local", n_queries=n_queries_ab, text_length=text_len_ab)
                else:
                    resB=benchmark_model(modelB, source="HF", n_queries=n_queries_ab, text_length=text_len_ab, hf_token=hf_tokenB)

                st.session_state["AB_test"]={"A":resA,"B":resB}
                cleanup_tmp(["/tmp/modelA","/tmp/modelB"])
                st.success("A/B —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ A/B —Ç–µ—Å—Ç–∞: {e}")

# ---------- Display results ----------

# Single results
if "bench_results" in st.session_state and st.session_state.bench_results:
    st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Single —Ç–µ—Å—Ç–æ–≤")
    df=pd.DataFrame(st.session_state.bench_results)
    st.dataframe(df)
    try:
        fig1=px.scatter(df,x="model_size_mb",y="time_batch_sec",size="ram_after_load_mb",color="model_id_or_path",
                        labels={"model_size_mb":"Model size (MB)","time_batch_sec":"Batch time (s)"},
                        title="–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ vs –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞")
        st.plotly_chart(fig1,use_container_width=True)
        fig2=px.scatter(df,x="num_parameters",y="time_single_ms",color="model_id_or_path",
                        labels={"num_parameters":"# parameters","time_single_ms":"Single request (ms)"},
                        title="–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ vs Single latency")
        st.plotly_chart(fig2,use_container_width=True)
    except Exception as e:
        st.write("–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤:",e)
    st.subheader("üõ† –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    for r in st.session_state.bench_results:
        st.markdown(f"### {r.get('model_id_or_path')} ‚Äî {r.get('timestamp')}")
        st.write({
            "Load time (s)": r.get("load_time_sec"),
            "Model size (MB)": r.get("model_size_mb"),
            "RAM after load (MB)": r.get("ram_after_load_mb"),
            "Embedding dim": r.get("embedding_dim"),
            "Num params": r.get("num_parameters"),
            "Num layers": r.get("num_layers"),
            "Time single (ms)": r.get("time_single_ms"),
            "Time batch (s)": r.get("time_batch_sec"),
            "Quantization (bitsandbytes)": r.get("quantization_bitsandbytes_available"),
            "FP16 CUDA": r.get("fp16_cuda_available"),
            "HF tags / languages": r.get("hf_tags", r.get("hf_languages"))
        })
        st.markdown("**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**")
        st.code(optimization_tips(r))

# A/B test results
if st.session_state.get("AB_test"):
    resA=st.session_state["AB_test"].get("A")
    resB=st.session_state["AB_test"].get("B")
    if resA and resB:
        resA_norm=normalize_result(resA)
        resB_norm=normalize_result(resB)
        df_ab=pd.DataFrame([resA_norm,resB_norm])
        df_ab.index=["A","B"]
        st.subheader("### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã A/B —Ç–µ—Å—Ç–∞")
        st.dataframe(df_ab)

        diff={}
        for m in resA_norm.keys():
            a_val=resA_norm.get(m)
            b_val=resB_norm.get(m)
            diff[m]={"A":a_val,"B":b_val,"diff (B-A)":(b_val-a_val) if a_val is not None and b_val is not None else None}
        st.markdown("### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫")
        st.dataframe(pd.DataFrame(diff).T)

        try:
            plot_df=pd.DataFrame([{"model":"A",**resA_norm},{"model":"B",**resB_norm}])
            fig=px.bar(plot_df,x="model",y=["load_time_sec","time_batch_sec","ram_after_load_mb"],
                       barmode="group", title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π A vs B")
            st.plotly_chart(fig,use_container_width=True)
        except Exception as e:
            st.write("–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ A/B:",e)
