# app.py
import streamlit as st
import time
import os
import psutil
import requests
import functools
import gdown
import zipfile
import shutil
import pandas as pd
import plotly.express as px
from sentence_transformers import SentenceTransformer
import torch

# –°–ª–æ–≤–∞—Ä—å —Å —Ä—É—Å—Å–∫–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–æ–ª–æ–Ω–æ–∫
COLUMN_NAMES_RU = {
    "model_id_or_path": "–ú–æ–¥–µ–ª—å",
    "source": "–ò—Å—Ç–æ—á–Ω–∏–∫",
    "load_time_sec": "–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ (—Å–µ–∫)",
    "ram_after_load_mb": "RAM –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ (–ú–ë)",
    "model_size_mb": "–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (–ú–ë)",
    "embedding_dim": "–†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞",
    "num_parameters": "–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤",
    "num_layers": "–°–ª–æ—ë–≤",
    "batch_optimized": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–µ–π",
    "quantization_bitsandbytes_available": "–î–æ—Å—Ç—É–ø–Ω–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ",
    "fp16_cuda_available": "FP16 CUDA",
    "hf_author": "–ê–≤—Ç–æ—Ä HF",
    "hf_lastModified": "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ HF",
    "hf_tags": "–¢–µ–≥–∏ HF",
    "hf_languages": "–Ø–∑—ã–∫–∏ HF",
    "time_single_ms": "–í—Ä–µ–º—è 1 –∑–∞–ø—Ä–æ—Å–∞ (–º—Å)",
    "time_batch_sec": "–í—Ä–µ–º—è –±–∞—Ç—á–∞ (—Å–µ–∫)",
    "avg_per_query_ms": "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º—Å)",
    "cpu_percent_sample": "CPU (%)",
    "timestamp": "–í—Ä–µ–º—è —Ç–µ—Å—Ç–∞"
}
# ---------- Helper utilities ----------

def sizeof_fmt(num, suffix="B"):
    for unit in ["", "K", "M", "G", "T", "P"]:
        if abs(num) < 1024.0:
            return f"{num:3.1f}{unit}{suffix}"
        num /= 1024.0
    return f"{num:.1f}P{suffix}"

def get_ram_usage_mb():
    proc = psutil.Process(os.getpid())
    return proc.memory_info().rss / (1024 ** 2)

def path_size_bytes(path):
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for f in filenames:
            try:
                fp = os.path.join(dirpath, f)
                total += os.path.getsize(fp)
            except Exception:
                pass
    return total

def try_import(name):
    try:
        return __import__(name)
    except Exception:
        return None

# ---------- HF API helpers ----------

def hf_model_info(model_name, token=None):
    try:
        headers = {}
        if token:
            headers["Authorization"] = f"Bearer {token}"
        url = f"https://huggingface.co/api/models/{model_name}"
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return None

def hf_model_files(model_name, token=None):
    try:
        headers = {}
        if token:
            headers["Authorization"] = f"Bearer {token}"
        url = f"https://huggingface.co/api/models/{model_name}/revision/main/files"
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return None

# ---------- Model loading (cached) ----------

@functools.lru_cache(maxsize=8)
def load_model_cached(path_or_name: str, from_hf: bool = True, hf_token: str = None):
    if from_hf and hf_token:
        model = SentenceTransformer(path_or_name, use_auth_token=hf_token)
    else:
        model = SentenceTransformer(path_or_name)
    return model

# ---------- Introspection helpers ----------

def inspect_model_info(model):
    info = {}
    try:
        emb = model.encode("—Ç–µ—Å—Ç", convert_to_tensor=True)
        info["dim"] = int(emb.shape[-1])
    except Exception:
        info["dim"] = None
    try:
        total = 0
        for p in model.parameters():
            total += p.numel()
        info["params"] = int(total)
    except Exception:
        info["params"] = None

    num_layers = None
    try:
        first = None
        try:
            first = model._first_module()
        except Exception:
            mods = list(model._modules.values())
            if mods:
                first = mods[0]
        if first is not None:
            auto_model = getattr(first, "auto_model", None) or getattr(first, "model", None) or getattr(first, "hf_model", None)
            if auto_model is not None:
                cfg = getattr(auto_model, "config", None)
                if cfg is not None:
                    num_layers = getattr(cfg, "num_hidden_layers", None) or getattr(cfg, "num_layers", None)
    except Exception:
        num_layers = None
    info["num_layers"] = int(num_layers) if num_layers is not None else None

    info["batch_optimized"] = True
    bnb = try_import("bitsandbytes")
    info["quantization_bitsandbytes_available"] = bool(bnb)
    info["fp16_cuda_available"] = torch.cuda.is_available()
    try:
        info["model_cache_folder"] = getattr(model, "cache_folder", None)
    except Exception:
        info["model_cache_folder"] = None
    return info

# ---------- Benchmarking ----------

def benchmark_model(model_name_or_path, source="HF", n_queries=10, text_length="short", hf_token=None):
    m = {}
    m["model_id_or_path"] = model_name_or_path
    m["source"] = source
    hf_meta = None
    if source == "HF":
        hf_meta = hf_model_info(model_name_or_path, token=hf_token)
    t0 = time.time()
    try:
        model = load_model_cached(model_name_or_path, from_hf=(source=="HF"), hf_token=hf_token)
        load_time = time.time() - t0
        m["load_time_sec"] = round(load_time, 3)
    except Exception as e:
        m["error"] = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}"
        return m
    try:
        m["ram_after_load_mb"] = round(get_ram_usage_mb(), 2)
    except Exception:
        m["ram_after_load_mb"] = None

    try:
        size_mb = None
        if source != "HF" and os.path.isdir(model_name_or_path):
            size_mb = path_size_bytes(model_name_or_path) / (1024*1024)
        else:
            cache_folder = getattr(model, "cache_folder", None)
            if cache_folder:
                sub = model_name_or_path.replace("/", "_")
                candidate = os.path.join(cache_folder, sub)
                if os.path.isdir(candidate):
                    size_mb = path_size_bytes(candidate) / (1024*1024)
                else:
                    size_mb = path_size_bytes(cache_folder) / (1024*1024)
        if size_mb is None and source == "HF":
            files = hf_model_files(model_name_or_path, token=hf_token)
            if isinstance(files, list):
                total = sum([f.get("size",0) if isinstance(f, dict) else 0 for f in files])
                size_mb = total / (1024*1024) if total>0 else None
        m["model_size_mb"] = round(size_mb,2) if size_mb else None
    except Exception:
        m["model_size_mb"] = None

    try:
        info = inspect_model_info(model)
        m.update({
            "embedding_dim": info.get("dim"),
            "num_parameters": int(info.get("params")) if info.get("params") else None,
            "num_layers": info.get("num_layers"),
            "batch_optimized": info.get("batch_optimized"),
            "quantization_bitsandbytes_available": info.get("quantization_bitsandbytes_available"),
            "fp16_cuda_available": info.get("fp16_cuda_available")
        })
    except Exception:
        pass

    if hf_meta:
        try:
            m["hf_author"] = hf_meta.get("author")
            m["hf_lastModified"] = hf_meta.get("lastModified")
            tags = hf_meta.get("tags") or []
            langs = hf_meta.get("languages") or []
            m["hf_tags"] = ", ".join(tags) if tags else None
            m["hf_languages"] = ", ".join(langs) if langs else None
        except Exception:
            pass

    if text_length=="short":
        sample_text="–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
    elif text_length=="medium":
        sample_text="–≠—Ç–æ —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–æ–¥–µ–ª—å—é."
    else:
        sample_text=" ".join(["–î–ª–∏–Ω–Ω—ã–π"]*200)

    try:
        _ = model.encode("—Ç—ë—Å—Ç", convert_to_tensor=True)
    except Exception:
        pass
    try:
        t1 = time.time()
        _ = model.encode(sample_text, convert_to_tensor=True)
        t_single = time.time() - t1
        m["time_single_ms"] = round(t_single*1000,3)
    except Exception:
        m["time_single_ms"] = None

    try:
        texts=[sample_text]*max(1,int(n_queries))
        t2=time.time()
        _=model.encode(texts, convert_to_tensor=True)
        t_batch=time.time()-t2
        m["time_batch_sec"]=round(t_batch,3)
        m["avg_per_query_ms"]=round((t_batch/len(texts))*1000,3)
    except Exception:
        m["time_batch_sec"]=None
        m["avg_per_query_ms"]=None

    try:
        cpu_before=psutil.cpu_percent(interval=None)
        _ = model.encode([sample_text]*5, convert_to_tensor=True)
        cpu_after=psutil.cpu_percent(interval=None)
        m["cpu_percent_sample"]=round((cpu_before+cpu_after)/2,2)
    except Exception:
        m["cpu_percent_sample"]=None

    return m

# ---------- Optimization tips ----------

def optimization_tips(result):
    tips=[]
    size=result.get("model_size_mb")
    params=result.get("num_parameters")
    ram=result.get("ram_after_load_mb")
    dim=result.get("embedding_dim")
    quant=result.get("quantization_bitsandbytes_available")
    fp16=result.get("fp16_cuda_available")

    if size and size>700:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å –±–æ–ª—å—à–∞—è (>700MB). –ù–∞ Streamlit Free –≤–æ–∑–º–æ–∂–Ω—ã OOM/–ø–∞–¥–µ–Ω–∏—è.")
    elif size and size>300:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (300‚Äì700MB). –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥–∏ –∑–∞ RAM.")
    elif size:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è ‚Äî –¥–æ–ª–∂–Ω–∞ –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –∏ —Ä–∞–±–æ—Ç–∞—Ç—å –ª—É—á—à–µ –Ω–∞ Free tier.")

    if params and params>200_000_000:
        tips.append("‚Ä¢ –ú–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (>200M) ‚Äî –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ CPU.")
    elif params and params>80_000_000:
        tips.append("‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —É–º–µ—Ä–µ–Ω–Ω–æ–µ (80‚Äì200M). –¢–µ—Å—Ç–∏—Ä—É–π batch_size.")

    if ram and ram>900:
        tips.append("‚Ä¢ –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª—å –∑–∞–Ω–∏–º–∞–µ—Ç –º–Ω–æ–≥–æ RAM (>900MB).")
    if quant:
        tips.append("‚Ä¢ bitsandbytes –¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å INT8/4bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ.")
    else:
        tips.append("‚Ä¢ bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.")
    if fp16:
        tips.append("‚Ä¢ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed-precision (fp16).")
    else:
        tips.append("‚Ä¢ CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî fp16 –Ω–µ –ø–æ–º–æ–∂–µ—Ç –Ω–∞ —ç—Ç–æ–π –º–∞—à–∏–Ω–µ.")
    if dim and dim>=1024:
        tips.append("‚Ä¢ –ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (>=1024) ‚Äî –ø–æ–≤—ã—à–µ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏.")
    elif dim and dim<=384:
        tips.append("‚Ä¢ –ú–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (<=384) ‚Äî —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å, —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ.")

    if size and size>700 or (ram and ram>900) or (params and params>200_000_000):
        tips.append("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ <500MB –∏–ª–∏ quantized/distilled –≤–µ—Ä—Å–∏–∏.")
    else:
        tips.append("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –º–æ–¥–µ–ª—å –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è Streamlit Free —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é.")

    return "\n".join(tips)

# ---------- Download GDrive ----------

def download_gdrive_model(file_id, dest_folder):
    dest_folder = f"/tmp/{file_id}"  # –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ
    os.makedirs(dest_folder, exist_ok=True)
    zip_path=os.path.join(dest_folder,"model.zip")
    url=f"https://drive.google.com/uc?id={file_id}"
    gdown.download(url, output=zip_path, quiet=False)
    with zipfile.ZipFile(zip_path,'r') as zip_ref:
        zip_ref.extractall(dest_folder)
    return dest_folder

# ---------- Cleanup tmp with logging ----------

def cleanup_tmp(folder_list):
    for folder in folder_list:
        if os.path.exists(folder):
            start=time.time()
            size_before=path_size_bytes(folder)
            try:
                shutil.rmtree(folder)
                elapsed=time.time()-start
                st.info(f"–û—á–∏—Å—Ç–∫–∞ {folder}: {sizeof_fmt(size_before)}, –≤—Ä–µ–º—è {elapsed:.2f}s")
            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {folder}: {e}")

# ---------- Streamlit UI ----------

st.set_page_config(layout="wide", page_title="Model Benchmark & Optimizer")
st.title("üîé Model Benchmark & Optimizer (HF + GDrive)")

# –¢–æ–ª—å–∫–æ Single —Ä–µ–∂–∏–º
col1,col2=st.columns([1,2])
with col1:
    source=st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏:", ["HuggingFace","GDrive"])
    if source=="HuggingFace":
        model_input=st.text_input("HF model id:", value="deepvk/USER-bge-m3")
        use_token=st.checkbox("–ü—Ä–∏–≤–∞—Ç–Ω—ã–π HF (use token from st.secrets['HUGGINGFACE_TOKEN'])", value=False)
        hf_token=st.secrets.get("HUGGINGFACE_TOKEN") if use_token else None
    else:
        model_input=st.text_input("GDrive File ID –º–æ–¥–µ–ª–∏:", value="1bZoykt0Sj2GRPvLRC_3Z7Wt8g4AGT34R")
        hf_token=None
    n_queries=st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ (batch) –¥–ª—è —Ç–µ—Å—Ç–∞:", min_value=1,max_value=1000,value=10,step=1)
    text_len=st.selectbox("–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞:", ["short","medium","long"])
    run_btn=st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç")

with col2:
    st.markdown("**–°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**")
    if "bench_results" not in st.session_state:
        st.session_state.bench_results=[]

if run_btn:
    # –ù–ï –æ—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if "bench_results" not in st.session_state:
        st.session_state.bench_results = []

    with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±–µ–Ω—á–º–∞—Ä–∫..."):
        try:
            start_ram = get_ram_usage_mb()
            start_time = time.time()

            if source=="GDrive":
                model_path=download_gdrive_model(model_input,f"/tmp/{model_input}")
                res=benchmark_model(model_path, source="Local", n_queries=n_queries, text_length=text_len)
            else:
                res=benchmark_model(model_input, source="HF", n_queries=n_queries, text_length=text_len, hf_token=hf_token)

            end_time = time.time()
            end_ram = get_ram_usage_mb()

            res["timestamp"]=time.strftime("%Y-%m-%d %H:%M:%S")
            st.session_state.bench_results.append(res)

            st.success("–¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω")
            st.info(f"RAM –¥–æ: {start_ram:.2f} MB, –ø–æ—Å–ª–µ: {end_ram:.2f} MB, —Ä–æ—Å—Ç: {end_ram - start_ram:.2f} MB, –≤—Ä–µ–º—è: {end_time-start_time:.2f}s")
            cleanup_tmp([f"/tmp/{model_input}"])
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")

# ---------- Display results ----------
if "bench_results" in st.session_state and st.session_state.bench_results:
    st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Single —Ç–µ—Å—Ç–æ–≤")
    df=pd.DataFrame(st.session_state.bench_results)
    df_display = df.rename(columns=COLUMN_NAMES_RU)
    st.dataframe(df_display)

    # –í—ã–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    selected_rows = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:",
        options=df.index.tolist(),
        format_func=lambda x: f"{df.loc[x,'model_id_or_path']} - {df.loc[x,'timestamp']}"
    )

    if st.button("–°—Ä–∞–≤–Ω–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ") and len(selected_rows) >= 2:
        compare_df = df.loc[selected_rows]
        st.dataframe(compare_df)
        try:
            fig=px.bar(compare_df, x="model_id_or_path", y=["load_time_sec","time_batch_sec","ram_after_load_mb"],
                       barmode="group", title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
            st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞: {e}")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    st.subheader("üõ† –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    for r in st.session_state.bench_results:
        st.markdown(f"### {r.get('model_id_or_path')} ‚Äî {r.get('timestamp')}")
        st.write({COLUMN_NAMES_RU.get(k, k): v for k, v in r.items()})
        st.markdown("**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**")
        st.code(optimization_tips(r))
