# app.py
import streamlit as st
import time
import os
import psutil
import requests
import json
import functools
import math
import gdown
import zipfile
import plotly.express as px

from sentence_transformers import SentenceTransformer
import torch
import plotly.express as px

# ---------- Helper utilities ----------

def sizeof_fmt(num, suffix="B"):
    # human readable
    for unit in ["", "K", "M", "G", "T", "P"]:
        if abs(num) < 1024.0:
            return f"{num:3.1f}{unit}{suffix}"
        num /= 1024.0
    return f"{num:.1f}P{suffix}"

def get_ram_usage_mb():
    proc = psutil.Process(os.getpid())
    return proc.memory_info().rss / (1024 ** 2)

def path_size_bytes(path):
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for f in filenames:
            try:
                fp = os.path.join(dirpath, f)
                total += os.path.getsize(fp)
            except Exception:
                pass
    return total

def try_import(name):
    try:
        return __import__(name)
    except Exception:
        return None

# ---------- HF API helpers ----------

def hf_model_info(model_name, token=None):
    """Try to get model metadata from HF api. Returns dict or None."""
    try:
        headers = {}
        if token:
            headers["Authorization"] = f"Bearer {token}"
        url = f"https://huggingface.co/api/models/{model_name}"
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return None

def hf_model_files(model_name, token=None):
    """Try to get list of repo files (and sizes) via HF API. Returns list or None."""
    # best-effort: use models/{model}/files endpoint (works for many public repos)
    try:
        headers = {}
        if token:
            headers["Authorization"] = f"Bearer {token}"
        url = f"https://huggingface.co/api/models/{model_name}/revision/main/files"
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return None

# ---------- Model loading (cached) ----------

@functools.lru_cache(maxsize=8)
def load_model_cached(path_or_name: str, from_hf: bool = True, hf_token: str = None):
    """Load SentenceTransformer model. from_hf True => HF repo id, else local folder path."""
    # If HF private token needed, pass use_auth_token in constructor
    if from_hf and hf_token:
        model = SentenceTransformer(path_or_name, use_auth_token=hf_token)
    else:
        model = SentenceTransformer(path_or_name)
    return model

# ---------- Introspection helpers ----------

def inspect_model_info(model, model_source_path=None):
    """
    Try to extract:
    - embedding dim
    - number of params
    - number of layers (if available)
    - batch optimization (True/False)
    - quantization support (bitsandbytes installed)
    - mixed precision support (fp16 GPU available)
    """
    info = {}
    # dim
    try:
        # do a single encode to get embedding dim if not obvious
        emb = model.encode("—Ç–µ—Å—Ç", convert_to_tensor=True)
        info["dim"] = int(emb.shape[-1])
    except Exception:
        info["dim"] = None

    # params
    try:
        total = 0
        for p in model.parameters():
            total += p.numel()
        info["params"] = int(total)
    except Exception:
        info["params"] = None

    # number of hidden layers - try to find transformer config
    num_layers = None
    try:
        # try several common attributes
        # some SentenceTransformer pipelines keep a huggingface model at model._first_module().auto_model
        first = None
        try:
            first = model._first_module()
        except Exception:
            # fallback: take first module of model._modules
            mods = list(model._modules.values())
            if mods:
                first = mods[0]
        if first is not None:
            auto_model = getattr(first, "auto_model", None) or getattr(first, "model", None) or getattr(first, "hf_model", None)
            if auto_model is not None:
                cfg = getattr(auto_model, "config", None)
                if cfg is not None:
                    num_layers = getattr(cfg, "num_hidden_layers", None) or getattr(cfg, "num_layers", None)
    except Exception:
        num_layers = None
    info["num_layers"] = int(num_layers) if num_layers is not None else None

    # batch optimization - sentence-transformers supports batching
    info["batch_optimized"] = True

    # quantization support (bitsandbytes)
    bnb = try_import("bitsandbytes")
    info["quantization_bitsandbytes_available"] = bool(bnb)

    # mixed precision (fp16) support: check CUDA presence
    info["fp16_cuda_available"] = torch.cuda.is_available()

    # model local path (if loaded from local)
    try:
        local_dir = getattr(model, "cache_folder", None)
        info["model_cache_folder"] = local_dir
    except Exception:
        info["model_cache_folder"] = None

    return info

# ---------- Benchmarking ----------

def benchmark_model(model_name_or_path, source="HF", n_queries=10, text_length="short", hf_token=None):
    """Performs a set of measurements and returns a dict of metrics and metadata."""
    m = {}
    m["model_id_or_path"] = model_name_or_path
    m["source"] = source

    # HF metadata
    hf_meta = None
    if source == "HF":
        try:
            hf_meta = hf_model_info(model_name_or_path, token=hf_token)
        except Exception:
            hf_meta = None

    # start measuring load time and RAM
    t0 = time.time()
    try:
        model = load_model_cached(model_name_or_path, from_hf=(source=="HF"), hf_token=hf_token)
        load_time = time.time() - t0
        m["load_time_sec"] = round(load_time, 3)
    except Exception as e:
        m["error"] = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}"
        return m

    # RAM after load
    try:
        m["ram_after_load_mb"] = round(get_ram_usage_mb(), 2)
    except Exception:
        m["ram_after_load_mb"] = None

    # model size (try local cache folder, else remote HF files)
    try:
        size_mb = None
        # prefer local model folder (if user gave local path or model cached)
        # if path exists and is dir:
        if source != "HF" and os.path.isdir(model_name_or_path):
            size_mb = path_size_bytes(model_name_or_path) / (1024 * 1024)
        else:
            # try to detect local cache folder reported by SentenceTransformer
            cache_folder = getattr(model, "cache_folder", None)
            if cache_folder:
                # construct probable subfolder name
                sub = model_name_or_path.replace("/", "_")
                candidate = os.path.join(cache_folder, sub)
                if os.path.isdir(candidate):
                    size_mb = path_size_bytes(candidate) / (1024 * 1024)
                else:
                    # maybe model placed directly in cache_folder
                    size_mb = path_size_bytes(cache_folder) / (1024 * 1024)
        # if still None, try via HF files API (best-effort)
        if size_mb is None and source == "HF":
            files = hf_model_files(model_name_or_path, token=hf_token)
            if isinstance(files, list):
                total = 0
                for f in files:
                    # file entries may include 'size' or not
                    sz = f.get("size", 0) if isinstance(f, dict) else 0
                    total += sz
                if total > 0:
                    size_mb = total / (1024 * 1024)
        m["model_size_mb"] = round(size_mb, 2) if size_mb else None
    except Exception:
        m["model_size_mb"] = None

    # inspect model internals
    try:
        info = inspect_model_info(model)
        m.update({
            "embedding_dim": info.get("dim"),
            "num_parameters": int(info.get("params")) if info.get("params") else None,
            "num_layers": info.get("num_layers"),
            "batch_optimized": info.get("batch_optimized"),
            "quantization_bitsandbytes_available": info.get("quantization_bitsandbytes_available"),
            "fp16_cuda_available": info.get("fp16_cuda_available")
        })
    except Exception:
        pass

    # HF meta fields (attempt)
    try:
        if hf_meta:
            m["hf_author"] = hf_meta.get("author")
            m["hf_lastModified"] = hf_meta.get("lastModified")
            # tags often contain languages or 'multilingual'
            tags = hf_meta.get("tags") or []
            # languages maybe present in 'pipeline_tag' or model card
            langs = hf_meta.get("languages") or []
            m["hf_tags"] = ", ".join(tags) if tags else None
            m["hf_languages"] = ", ".join(langs) if langs else None
    except Exception:
        pass

    # now perform encoding benchmarks
    if text_length == "short":
        sample_text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
    elif text_length == "medium":
        sample_text = "–≠—Ç–æ —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–æ–¥–µ–ª—å—é."
    else:
        sample_text = " ".join(["–î–ª–∏–Ω–Ω—ã–π"] * 200)

    # warmup
    try:
        _ = model.encode("—Ç—ë—Å—Ç", convert_to_tensor=True)
    except Exception:
        pass

    # single request
    try:
        t1 = time.time()
        _ = model.encode(sample_text, convert_to_tensor=True)
        t_single = time.time() - t1
        m["time_single_ms"] = round(t_single * 1000, 3)
    except Exception:
        m["time_single_ms"] = None

    # batch
    try:
        texts = [sample_text] * max(1, int(n_queries))
        t2 = time.time()
        _ = model.encode(texts, convert_to_tensor=True)
        t_batch = time.time() - t2
        m["time_batch_sec"] = round(t_batch, 3)
        m["avg_per_query_ms"] = round((t_batch / len(texts)) * 1000, 3)
    except Exception:
        m["time_batch_sec"] = None
        m["avg_per_query_ms"] = None

    # CPU load (short sample over encoding)
    try:
        cpu_before = psutil.cpu_percent(interval=None)
        # do a small encode
        _ = model.encode([sample_text] * 5, convert_to_tensor=True)
        cpu_after = psutil.cpu_percent(interval=None)
        m["cpu_percent_sample"] = round((cpu_before + cpu_after) / 2, 2)
    except Exception:
        m["cpu_percent_sample"] = None

    return m

# ---------- Optimization recommendations for Streamlit Free ----------

def optimization_tips(result):
    """
    Produce a text block of recommendations based on metrics and thresholds.
    """
    tips = []
    size = result.get("model_size_mb")
    params = result.get("num_parameters")
    ram = result.get("ram_after_load_mb")
    dim = result.get("embedding_dim")
    quant = result.get("quantization_bitsandbytes_available")
    fp16 = result.get("fp16_cuda_available")

    # thresholds suitable for Streamlit Free (approx)
    if size and size > 700:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å –±–æ–ª—å—à–∞—è (>700MB). –ù–∞ Streamlit Free –≤–æ–∑–º–æ–∂–Ω—ã OOM/–ø–∞–¥–µ–Ω–∏—è. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: distill/quantize/–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ª—ë–≥–∫—É—é –º–æ–¥–µ–ª—å.")
    elif size and size > 300:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (300‚Äì700MB). –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥–∏ –∑–∞ RAM –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏ –±–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–∏–∏.")
    elif size:
        tips.append("‚Ä¢ –ú–æ–¥–µ–ª—å –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è ‚Äî –¥–æ–ª–∂–Ω–∞ –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –∏ —Ä–∞–±–æ—Ç–∞—Ç—å –ª—É—á—à–µ –Ω–∞ Free tier.")

    if params and params > 200_000_000:
        tips.append("‚Ä¢ –ú–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (>200M) ‚Äî –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ CPU.")
    elif params and params > 80_000_000:
        tips.append("‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —É–º–µ—Ä–µ–Ω–Ω–æ–µ (80‚Äì200M). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å batch_size –∏ throttle.")

    if ram and ram > 900:
        tips.append("‚Ä¢ –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª—å –∑–∞–Ω–∏–º–∞–µ—Ç –º–Ω–æ–≥–æ RAM (>900MB). –ù–∞ Streamlit Free —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ ‚Äî –ø–æ–¥—É–º–∞–π –æ quantization –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–º —Ö–æ—Å—Ç–∏–Ω–≥–µ.")
    if quant:
        tips.append("‚Ä¢ bitsandbytes –¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å INT8/4bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (—É–º–µ–Ω—å—à–∏—Ç RAM –∏ —É—Å–∫–æ—Ä–∏—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å).")
    else:
        tips.append("‚Ä¢ bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ. –î–ª—è CPU –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å `torch.quantization`/distillation.")

    if fp16:
        tips.append("‚Ä¢ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed-precision (fp16) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ GPU.")
    else:
        tips.append("‚Ä¢ CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî fp16 –Ω–µ –ø–æ–º–æ–∂–µ—Ç –Ω–∞ —ç—Ç–æ–π –º–∞—à–∏–Ω–µ.")

    if dim and dim >= 1024:
        tips.append("‚Ä¢ –ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (>=1024) ‚Äî –ø–æ–≤—ã—à–µ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –Ω–æ –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –≤ –∏–Ω–¥–µ–∫—Å–µ (FAISS/pgvector).")
    elif dim and dim <= 384:
        tips.append("‚Ä¢ –ú–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (<=384) ‚Äî —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å, –Ω–æ —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∏–∂–µ.")

    # final advice
    if size and size > 700 or (ram and ram > 900) or (params and params > 200_000_000):
        tips.append("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–æ–º Streamlit –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ < 500MB –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å quantized/distilled –≤–µ—Ä—Å–∏–∏. –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ persistent storage –∏ –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å—Ç–∞—Ä—Ç–µ.")
    else:
        tips.append("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –º–æ–¥–µ–ª—å –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è Streamlit Free —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é ‚Äî –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π –Ω–∞–≥—Ä—É–∑–∫—É (N requests) –∏ —É–º–µ–Ω—å—à–∞–π batch_size –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.")

    return "\n".join(tips)

# ---------- Streamlit UI ----------

st.set_page_config(layout="wide", page_title="Model Benchmark & Optimizer")

st.title("üîé Model Benchmark & Optimizer (HF + GDrive)")

col1, col2 = st.columns([1, 2])

with col1:
    source = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏:", ["HuggingFace", "Local (GDrive / path)"])
    if source == "HuggingFace":
        model_input = st.text_input("HF model id:", value="deepvk/USER-bge-m3")
        use_token = st.checkbox("–ü—Ä–∏–≤–∞—Ç–Ω—ã–π HF (use token from st.secrets['HUGGINGFACE_TOKEN'])", value=False)
        hf_token = st.secrets.get("HUGGINGFACE_TOKEN") if use_token else None
    else:
        model_input = st.text_input("–ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (GDrive):", value="/content/drive/MyDrive/models/my_model")
        hf_token = None

    n_queries = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ (batch) –¥–ª—è —Ç–µ—Å—Ç–∞:", min_value=1, max_value=1000, value=10, step=1)
    text_len = st.selectbox("–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞:", ["short", "medium", "long"])
    run_btn = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç")

with col2:
    st.markdown("**–°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**")
    if "bench_results" not in st.session_state:
        st.session_state.bench_results = []

if run_btn:
    with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±–µ–Ω—á–º–∞—Ä–∫... –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏"):
        res = benchmark_model(model_input, source=("HF" if source=="HuggingFace" else "Local"), n_queries=n_queries, text_length=text_len, hf_token=hf_token)
    # append timestamp
    res["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    st.session_state.bench_results.append(res)
    st.success("–¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω")
st.markdown("---")
st.subheader("‚öñÔ∏è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")

colA, colB = st.columns(2)

with colA:
    source_a = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏ A:", ["HF", "GDrive"], key="sourceA")
    if source_a == "HF":
        modelA = st.text_input("–ú–æ–¥–µ–ª—å A (HF ID):", "deepvk/USER-bge-m3", key="modelA")
        hf_tokenA = st.text_input("HF Token –¥–ª—è –º–æ–¥–µ–ª–∏ A (–µ—Å–ª–∏ –ø—Ä–∏–≤–∞—Ç–Ω–∞—è):", type="password", key="hfA")
    else:
        modelA_file_id = st.text_input("Google Drive File ID –º–æ–¥–µ–ª–∏ A:", "1RR15OMLj9vfSrVa1HN-dRU-4LbkdbRRf", key="gdriveA")

with colB:
    source_b = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏ B:", ["HF", "GDrive"], key="sourceB")
    if source_b == "HF":
        modelB = st.text_input("–ú–æ–¥–µ–ª—å B (HF ID):", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", key="modelB")
        hf_tokenB = st.text_input("HF Token –¥–ª—è –º–æ–¥–µ–ª–∏ B (–µ—Å–ª–∏ –ø—Ä–∏–≤–∞—Ç–Ω–∞—è):", type="password", key="hfB")
    else:
        modelB_file_id = st.text_input("Google Drive File ID –º–æ–¥–µ–ª–∏ B:", "1RR15OMLj9vfSrVa1HN-dRU-4LbkdbRRf", key="gdriveB")

n_queries_ab = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è A/B:", min_value=1, max_value=500, value=10, key="n_queries_ab")
text_len_ab = st.selectbox("–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è A/B:", ["short", "medium", "long"], key="text_len_ab")

# ---------- —Ñ—É–Ω–∫—Ü–∏–∏ ----------
def download_gdrive_model(file_id, dest_folder):
    os.makedirs(dest_folder, exist_ok=True)
    zip_path = os.path.join(dest_folder, "model.zip")
    url = f"https://drive.google.com/uc?id={file_id}"
    gdown.download(url, output=zip_path, quiet=False)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(dest_folder)
    return dest_folder

def normalize_result(res):
    """
    –ü—Ä–∏–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç benchmark_model –∫ –ø–ª–æ—Å–∫–æ–º—É —Å–ª–æ–≤–∞—Ä—é –¥–ª—è DataFrame.
    –õ—é–±—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º None.
    """
    keys = ["load_time_sec","ram_after_load_mb","time_single_ms","time_batch_sec",
            "avg_per_query_ms","model_size_mb","embedding_dim","num_parameters"]
    if not res or "error" in res:
        return {k: None for k in keys}
    return {k: res.get(k, None) for k in keys}

# ---------- –∑–∞–ø—É—Å–∫ ----------
if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å A/B —Ç–µ—Å—Ç"):
    with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ–º A/B —Ç–µ—Å—Ç..."):
        # --- –º–æ–¥–µ–ª—å A ---
        try:
            if source_a == "GDrive":
                modelA_path = download_gdrive_model(modelA_file_id, "/tmp/modelA")
                resA = benchmark_model(modelA_path, source="Local", n_queries=n_queries_ab, text_length=text_len_ab)
            else:
                resA = benchmark_model(modelA, source="HF", n_queries=n_queries_ab, text_length=text_len_ab, hf_token=hf_tokenA)
        except Exception as e:
            resA = {"error": str(e)}

        # --- –º–æ–¥–µ–ª—å B ---
        try:
            if source_b == "GDrive":
                modelB_path = download_gdrive_model(modelB_file_id, "/tmp/modelB")
                resB = benchmark_model(modelB_path, source="Local", n_queries=n_queries_ab, text_length=text_len_ab)
            else:
                resB = benchmark_model(modelB, source="HF", n_queries=n_queries_ab, text_length=text_len_ab, hf_token=hf_tokenB)
        except Exception as e:
            resB = {"error": str(e)}

    # --- —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–µ—Å—Å–∏—é ---
    st.session_state["AB_test"] = {"A": resA, "B": resB}

# ---------- –≤—ã–≤–æ–¥ ----------
if st.session_state.get("AB_test"):
    resA_norm = normalize_result(st.session_state["AB_test"]["A"])
    resB_norm = normalize_result(st.session_state["AB_test"]["B"])
    
    df_ab = pd.DataFrame([resA_norm, resB_norm])
    df_ab.index = ["A","B"]
    st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã A/B —Ç–µ—Å—Ç–∞")
    st.dataframe(df_ab)

    st.markdown("### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫")
    metrics = list(resA_norm.keys())
    diff = {}
    for m in metrics:
        a_val = resA_norm.get(m)
        b_val = resB_norm.get(m)
        diff[m] = {"A": a_val, "B": b_val, "diff (B-A)": (b_val - a_val) if a_val is not None and b_val is not None else None}
    st.dataframe(pd.DataFrame(diff).T)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    try:
        plot_df = pd.DataFrame([
            {"model":"A", **resA_norm},
            {"model":"B", **resB_norm}
        ])
        fig = px.bar(plot_df, x="model", y=["load_time_sec","time_batch_sec","ram_after_load_mb"],
                     barmode="group", title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π A vs B")
        st.plotly_chart(fig, use_container_width=True)
    except Exception as e:
        st.write("–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ A/B:", e)

# display accumulated results
if st.session_state.get("bench_results"):
    df = st.session_state.bench_results.copy()

    # Normalize numeric fields for plotting (some may be None)
    for r in df:
        for k in ["load_time_sec", "model_size_mb", "ram_after_load_mb", "time_single_ms", "time_batch_sec", "avg_per_query_ms", "num_parameters"]:
            if k not in r:
                r[k] = None

    st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤")
    st.dataframe(df)

    # Simple interactive filters
    st.subheader("üìà –ì—Ä–∞—Ñ–∏–∫–∏")
    try:
        # build small DataFrame for plotting
        import pandas as pd
        plot_df = pd.DataFrame(df)
        # size vs batch time
        fig1 = px.scatter(plot_df, x="model_size_mb", y="time_batch_sec", size="ram_after_load_mb", color="model_id_or_path",
                          labels={"model_size_mb":"Model size (MB)", "time_batch_sec":f"Batch time (sec) for {n_queries} queries"},
                          title="–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ vs –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞")
        st.plotly_chart(fig1, use_container_width=True)

        # params vs single latency
        if "num_parameters" in plot_df.columns:
            fig2 = px.scatter(plot_df, x="num_parameters", y="time_single_ms", color="model_id_or_path",
                              labels={"num_parameters":"# parameters", "time_single_ms":"Single request (ms)"},
                              title="–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ vs –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")
            st.plotly_chart(fig2, use_container_width=True)
    except Exception as e:
        st.write("–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤:", e)

    # Show detailed info + optimization tips per model
    st.subheader("üõ† –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Streamlit Free (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)")
    for r in df:
        st.markdown(f"### {r.get('model_id_or_path')}  ‚Äî  {r.get('timestamp')}")
        # pretty printing selected metrics
        st.write({
            "Load time (s)": r.get("load_time_sec"),
            "Model size (MB)": r.get("model_size_mb"),
            "RAM after load (MB)": r.get("ram_after_load_mb"),
            "Embedding dim": r.get("embedding_dim"),
            "Num params": r.get("num_parameters"),
            "Num layers": r.get("num_layers"),
            "Time single (ms)": r.get("time_single_ms"),
            "Time batch (s)": r.get("time_batch_sec"),
            "Quantization available (bitsandbytes)": r.get("quantization_bitsandbytes_available"),
            "FP16 CUDA available": r.get("fp16_cuda_available"),
            "HF tags / languages": r.get("hf_tags", r.get("hf_languages"))
        })
        st.markdown("**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**")
        st.code(optimization_tips(r))

st.markdown("---")
st.markdown("### üìù –ü—Ä–∏–º–µ—á–∞–Ω–∏—è –∏ —Å–æ–≤–µ—Ç—ã")
st.markdown("""
- –î–ª—è –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö HF –º–æ–¥–µ–ª–µ–π –¥–æ–±–∞–≤—å —Ç–æ–∫–µ–Ω –≤ `st.secrets['HUGGINGFACE_TOKEN']` –∏ –ø–æ—Å—Ç–∞–≤—å –≥–∞–ª–æ—á–∫—É '–ü—Ä–∏–≤–∞—Ç–Ω—ã–π HF'.
- –î–ª—è Streamlit Free –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ **< 500MB** (–∏–ª–∏ quantized/distilled –≤–µ—Ä—Å–∏–∏).  
- –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∞—è, –ª—É—á—à–µ **–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏** –¥–ª—è –∫–æ—Ä–ø—É—Å–∞ –∏ —Ö—Ä–∞–Ω–∏—Ç—å –∏—Ö –≤ persistent storage (S3/DB/pgvector), —á—Ç–æ–±—ã –Ω–µ –¥–µ—Ä–∂–∞—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –≤ –ø–∞–º—è—Ç–∏.
- –ï—Å–ª–∏ —É —Ç–µ–±—è –Ω–µ—Ç GPU, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å `bitsandbytes` –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–æ–ª–µ–∑–Ω–æ ‚Äî –Ω–æ INT8-quantized –≤–µ—Ä—Å–∏–∏ –∏–Ω–æ–≥–¥–∞ –∑–∞–º–µ—Ç–Ω–æ —Å–Ω–∏–∂–∞—é—Ç RAM.
- –¢–µ—Å—Ç–∏—Ä—É–π —Ä–∞–∑–Ω—ã–µ `n_queries` –∏ `text length` ‚Äî –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è.
""")
