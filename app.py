# app.py
import streamlit as st
import time
import os
import psutil
import requests
import functools
import gdown
import zipfile
import pandas as pd
import plotly.express as px
from sentence_transformers import SentenceTransformer
import torch
import shutil

# ---------- Helper utilities ----------

def sizeof_fmt(num, suffix="B"):
    for unit in ["", "K", "M", "G", "T", "P"]:
        if abs(num) < 1024.0:
            return f"{num:3.1f}{unit}{suffix}"
        num /= 1024.0
    return f"{num:.1f}P{suffix}"

def get_ram_usage_mb():
    proc = psutil.Process(os.getpid())
    return proc.memory_info().rss / (1024 ** 2)

def try_import(name):
    try:
        return __import__(name)
    except Exception:
        return None

# ---------- Model loading (cached) ----------

@functools.lru_cache(maxsize=8)
def load_model_cached(path_or_name: str, hf_token: str = None):
    if hf_token:
        model = SentenceTransformer(path_or_name, use_auth_token=hf_token)
    else:
        model = SentenceTransformer(path_or_name)
    return model

# ---------- Benchmarking ----------

def benchmark_model(model_name_or_path, source="HF", n_queries=10, text_length="short", hf_token=None):
    m = {}
    m["model_id_or_path"] = model_name_or_path
    m["source"] = source

    t0 = time.time()
    try:
        model = load_model_cached(model_name_or_path, hf_token=hf_token if source=="HF" else None)
        load_time = time.time() - t0
        m["load_time_sec"] = round(load_time,3)
    except Exception as e:
        m["error"] = str(e)
        return m

    m["ram_after_load_mb"] = round(get_ram_usage_mb(),2)
    try:
        sample_text = {"short":"ÐŸÑ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€", "medium":"Ð­Ñ‚Ð¾ Ñ‚ÐµÑÑ‚ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ.", "long":" ".join(["Ð”Ð»Ð¸Ð½Ð½Ñ‹Ð¹"]*200)}.get(text_length, "ÐŸÑ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€")
        _ = model.encode(sample_text, convert_to_tensor=True)
        t1 = time.time()
        _ = model.encode(sample_text, convert_to_tensor=True)
        m["time_single_ms"] = round((time.time()-t1)*1000,3)
        texts = [sample_text]*max(1,int(n_queries))
        t2 = time.time()
        _ = model.encode(texts, convert_to_tensor=True)
        t_batch = time.time()-t2
        m["time_batch_sec"] = round(t_batch,3)
        m["avg_per_query_ms"] = round((t_batch/len(texts))*1000,3)
    except Exception:
        m["time_single_ms"]=m["time_batch_sec"]=m["avg_per_query_ms"]=None

    # embedding dim and params
    try:
        emb = model.encode("Ñ‚ÐµÑÑ‚", convert_to_tensor=True)
        m["embedding_dim"] = int(emb.shape[-1])
    except: m["embedding_dim"] = None

    try:
        total = 0
        for p in model.parameters(): total+=p.numel()
        m["num_parameters"]=int(total)
    except: m["num_parameters"]=None

    # quantization support
    m["quantization_bitsandbytes_available"] = bool(try_import("bitsandbytes"))
    m["fp16_cuda_available"] = torch.cuda.is_available()

    return m

# ---------- GDrive helper ----------

def download_gdrive_model(file_id, dest_folder):
    os.makedirs(dest_folder, exist_ok=True)
    zip_path = os.path.join(dest_folder, "model.zip")
    url = f"https://drive.google.com/uc?id={file_id}"
    gdown.download(url, output=zip_path, quiet=False)
    with zipfile.ZipFile(zip_path,'r') as zip_ref:
        zip_ref.extractall(dest_folder)
    return dest_folder

# ---------- Cleanup with logging ----------

def cleanup_tmp_with_logging(folders=["/tmp/modelA","/tmp/modelB"]):
    for folder in folders:
        if os.path.exists(folder):
            try:
                start_ram = psutil.virtual_memory().used
                t0 = time.time()
                shutil.rmtree(folder)
                t1 = time.time()
                end_ram = psutil.virtual_memory().used
                freed = abs(end_ram - start_ram)
                st.info(f"Ð£Ð´Ð°Ð»ÐµÐ½Ð° Ð¿Ð°Ð¿ÐºÐ° {folder}. Ð’Ñ€ÐµÐ¼Ñ: {round(t1-t0,2)}s, Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸: {sizeof_fmt(freed)}")
            except Exception as e:
                st.warning(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð¿Ð°Ð¿ÐºÑƒ {folder}: {e}")

# ---------- Streamlit UI ----------

st.set_page_config(layout="wide", page_title="Model Benchmark & Optimizer")
st.title("ðŸ”Ž Model Benchmark & Optimizer (HF + GDrive)")

mode = st.radio("Ð ÐµÐ¶Ð¸Ð¼ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹:", ["Single", "A/B Ñ‚ÐµÑÑ‚"])

if "bench_results" not in st.session_state:
    st.session_state.bench_results = []

if mode=="Single":
    source = st.radio("Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸:", ["HuggingFace", "GDrive"])
    if source=="HuggingFace":
        model_input = st.text_input("HF model id:", value="deepvk/USER-bge-m3")
        use_token = st.checkbox("ÐŸÑ€Ð¸Ð²Ð°Ñ‚Ð½Ñ‹Ð¹ HF (use token from st.secrets['HUGGINGFACE_TOKEN'])", value=False)
        hf_token = st.secrets.get("HUGGINGFACE_TOKEN") if use_token else None
    else:
        model_input = st.text_input("Google Drive File ID Ð¼Ð¾Ð´ÐµÐ»Ð¸:", "1bZoykt0Sj2GRPvLRC_3Z7Wt8g4AGT34R")
        hf_token = None

    n_queries = st.number_input("ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² (batch) Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð°:", min_value=1,max_value=1000,value=10,step=1)
    text_len = st.selectbox("Ð”Ð»Ð¸Ð½Ð° Ñ‚ÐµÐºÑÑ‚Ð°:", ["short","medium","long"])
    run_btn = st.button("Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ñ‚ÐµÑÑ‚")

    if run_btn:
        with st.spinner("Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº..."):
            if source=="GDrive":
                model_path = download_gdrive_model(model_input,"/tmp/modelSingle")
                res = benchmark_model(model_path, source="Local", n_queries=n_queries, text_length=text_len)
                cleanup_tmp_with_logging(["/tmp/modelSingle"])
            else:
                res = benchmark_model(model_input, source="HF", n_queries=n_queries, text_length=text_len, hf_token=hf_token)
            res["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            st.session_state.bench_results.append(res)
        st.success("Ð¢ÐµÑÑ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½")

elif mode=="A/B Ñ‚ÐµÑÑ‚":
    colA,colB = st.columns(2)
    with colA:
        source_a = st.radio("Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸ A:", ["HF","GDrive"], key="sourceA")
        if source_a=="HF":
            modelA = st.text_input("ÐœÐ¾Ð´ÐµÐ»ÑŒ A (HF ID):","deepvk/USER-bge-m3", key="modelA")
            hf_tokenA = st.text_input("HF Token Ð¼Ð¾Ð´ÐµÐ»Ð¸ A (ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¸Ð²Ð°Ñ‚Ð½Ð°Ñ):", type="password", key="hfA")
        else:
            modelA_file_id = st.text_input("Google Drive File ID Ð¼Ð¾Ð´ÐµÐ»Ð¸ A:","1bZoykt0Sj2GRPvLRC_3Z7Wt8g4AGT34R", key="gdriveA")

    with colB:
        source_b = st.radio("Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸ B:", ["HF","GDrive"], key="sourceB")
        if source_b=="HF":
            modelB = st.text_input("ÐœÐ¾Ð´ÐµÐ»ÑŒ B (HF ID):","sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", key="modelB")
            hf_tokenB = st.text_input("HF Token Ð¼Ð¾Ð´ÐµÐ»Ð¸ B (ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¸Ð²Ð°Ñ‚Ð½Ð°Ñ):", type="password", key="hfB")
        else:
            modelB_file_id = st.text_input("Google Drive File ID Ð¼Ð¾Ð´ÐµÐ»Ð¸ B:","1bZoykt0Sj2GRPvLRC_3Z7Wt8g4AGT34R", key="gdriveB")

    n_queries_ab = st.number_input("ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²:", min_value=1,max_value=500,value=10,key="n_queries_ab")
    text_len_ab = st.selectbox("Ð”Ð»Ð¸Ð½Ð° Ñ‚ÐµÐºÑÑ‚Ð°:", ["short","medium","long"], key="text_len_ab")
    run_ab_btn = st.button("Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ A/B Ñ‚ÐµÑÑ‚")

    if run_ab_btn:
        with st.spinner("Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ A/B Ñ‚ÐµÑÑ‚..."):
            # --- Ð¼Ð¾Ð´ÐµÐ»ÑŒ A ---
            try:
                if source_a=="GDrive":
                    modelA_path = download_gdrive_model(modelA_file_id,"/tmp/modelA")
                    resA = benchmark_model(modelA_path, source="Local", n_queries=n_queries_ab, text_length=text_len_ab)
                else:
                    resA = benchmark_model(modelA, source="HF", n_queries=n_queries_ab, text_length=text_len_ab, hf_token=hf_tokenA)
            except Exception as e:
                resA = {"error": str(e)}

            # --- Ð¼Ð¾Ð´ÐµÐ»ÑŒ B ---
            try:
                if source_b=="GDrive":
                    modelB_path = download_gdrive_model(modelB_file_id,"/tmp/modelB")
                    resB = benchmark_model(modelB_path, source="Local", n_queries=n_queries_ab, text_length=text_len_ab)
                else:
                    resB = benchmark_model(modelB, source="HF", n_queries=n_queries_ab, text_length=text_len_ab, hf_token=hf_tokenB)
            except Exception as e:
                resB = {"error": str(e)}

            # append results
            for r in [resA,resB]:
                r["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                st.session_state.bench_results.append(r)

            cleanup_tmp_with_logging(["/tmp/modelA","/tmp/modelB"])
        st.success("A/B Ñ‚ÐµÑÑ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½")

# ---------- Ð²Ñ‹Ð²Ð¾Ð´ Ð²ÑÐµÑ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ----------
if st.session_state.get("bench_results"):
    df = pd.DataFrame(st.session_state.bench_results)
    st.subheader("ðŸ“‹ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ‚ÐµÑÑ‚Ð¾Ð²")
    st.dataframe(df)

    # Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ðµ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸
    try:
        fig1 = px.scatter(df, x="num_parameters", y="time_single_ms", color="model_id_or_path",
                          labels={"num_parameters":"# parameters","time_single_ms":"Single request (ms)"},
                          title="ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ vs Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°")
        st.plotly_chart(fig1,use_container_width=True)
    except Exception as e:
        st.write("ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ñ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð²:",e)

st.markdown("---")
st.markdown("### ðŸ“ ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ñ")
st.markdown("""
- Ð”Ð»Ñ Ð¿Ñ€Ð¸Ð²Ð°Ñ‚Ð½Ñ‹Ñ… HF Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð¾Ð±Ð°Ð²ÑŒ Ñ‚Ð¾ÐºÐµÐ½ Ð² `st.secrets['HUGGINGFACE_TOKEN']`.
- Ð”Ð»Ñ GDrive Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ File ID.
- Ð’ÑÐµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Single Ð¸ A/B Ñ‚ÐµÑÑ‚Ð° ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑŽÑ‚ÑÑ Ð² Ð¾Ð´Ð½Ð¾Ð¼ ÑÐ¿Ð¸ÑÐºÐµ.
""")
